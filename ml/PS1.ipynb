{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 1\n",
    "\n",
    "To run and solve this assignment, one must have a working IPython Notebook installation. The easiest way to set it up for both Windows and Linux is to install [Anaconda](https://www.continuum.io/downloads). Then save this file to your computer (use \"Raw\" link on gist\\github), run Anaconda and choose this file in Anaconda's file explorer. Use the `Python 3` version. Everything that follows assumes that you have already followed these instructions. If you are new to Python or its scientific library, Numpy, there are some nice tutorials [here](https://www.learnpython.org/) and [here](http://www.scipy-lectures.org/).\n",
    "\n",
    "To run code in a cell or to render [Markdown](https://en.wikipedia.org/wiki/Markdown)+[LaTeX](https://en.wikipedia.org/wiki/LaTeX) press `Ctr+Enter` or `[>|]`(like \"play\") button above. To edit any code or text cell [double]click on its content. To change cell type, choose \"Markdown\" or \"Code\" in the drop-down menu above.\n",
    "\n",
    "If certain output is given for some cells, that means that you are expected to get similar results.\n",
    "\n",
    "Total: 80 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Linear Algebra\n",
    "**1\\.1 [4pt]**  Given three square matrices $Q, R, S \\in R^{nÃ—n}$ , which statements are true in general?\n",
    "\n",
    "(a) $(QRS)^{-1} = S^{-1}R^{âˆ’1}Q^{âˆ’1}$\n",
    "\n",
    "(b) $QR = RQ$\n",
    "\n",
    "(c) $(QRS)^T = Q^T R^T S^T$\n",
    "\n",
    "(d) $Q(R + S) = QS + QR$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "\n",
    "a and d are true\n",
    "            \n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1\\.2 Vectors** \n",
    "\n",
    "**1\\.2\\.1 [1pt]** Given points $p_1 = (1, 6, 5)$ and $p_2 = (âˆ’2, 2, 5)$, solve for $v_1$ the vector from $p_1$ to $p_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "\n",
    "$v_1$ = [-3, -4, 0]\n",
    "            \n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1\\.2\\.2 [1pt]** Given a third point $p_3 = (0, 6, 5)$, solve for $v_2$ the vector from $p_1$ to $p_3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "\n",
    "$v_2$ = [-1, 0, 0]\n",
    "            \n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1\\.2\\.3 [1pt]** Find the value for the magnitude of $v_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "\n",
    "sqrt(9+16+0) = 5\n",
    "            \n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1\\.2\\.4 [1pt]**  Solve for the scalar (dot) product $v_2 \\cdot v_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "\n",
    "-3\\*-1 + -4\\*0 + 0\\*0 = 3\n",
    "            \n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1\\.2.5 [1pt]** If two vectors $u, v \\in R^n$ are orthogonal, what is the value of their scalar (dot) product?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "\n",
    "The value of their scalar product is 0, because the two vectors are perpendicular\n",
    "            \n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Partial Derivatives\n",
    "**2\\.1 [6pt]** Compute the partial derivative of $f$ with respect to each of: $x$, $y$, and $z$.\n",
    "$$f\\left( {x,y,z} \\right) = 4{x^3}{y^2} - {{\\bf{e}}^z}{y^4} + \\frac{{{z^3}}}{{{x^2}}} + 4y - {x^{16}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "\n",
    "$\\delta f / \\delta x = 12{y^2}{x^2} - 2{z^3}{x^{-3}} - 16x^{15}$  \n",
    "\n",
    "$\\delta f / \\delta y = 8{x^3}y - 4{e^z}{y^3} + 4$\n",
    "\n",
    "$\\delta f / \\delta z = -{y^4}{e^z} + 3{z^2}{x^{-2}}$\n",
    "\n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2\\.2 [2pt]** Using the chain rule, compute $ð‘“'(\\theta)$ (or $\\delta f / \\delta \\theta$), where $f$ is composed of chained functions $ð‘“(\\theta) = ð‘“_1(ð‘“_2(\\theta))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "\n",
    "$ð‘“(\\theta) = ð‘“_1'(ð‘“_2(\\theta))ð‘“_2'(\\theta)$\n",
    "            \n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2\\.3 [4pt]** Using the chain rule, compute the partial derivatives of $ð‘“(\\theta_1, \\theta_2) = ð‘“_1(ð‘“_2(\\theta_1, \\theta_2))$ with respect to each of: $\\theta_1$ and $\\theta_2$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "\n",
    "$\\delta f / \\delta \\theta_1 = \\delta / \\delta \\theta_1 ð‘“_1(ð‘“_2(\\theta_1, \\theta_2)) \\delta / \\delta \\theta_1 ð‘“_2(\\theta_1, \\theta_2)$\n",
    "\n",
    "\n",
    "$\\delta f / \\delta \\theta_2 = \\delta/ \\delta \\theta_2  ð‘“_1 (ð‘“_2(\\theta_1, \\theta_2)) \\delta / \\delta \\theta_2 ð‘“_2(\\theta_1, \\theta_2)$\n",
    "            \n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Gradient Descent\n",
    "With each step of gradient descent, your parameter $\\theta_j$ comes closer to the local minimum of the cost $J(Î¸)$.\n",
    "\n",
    "**3\\.1 [10pt]**\n",
    "Compute the partial derivative of the regression loss function \n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i = 1}^{m} \\big(h(x^{(i)}; \\theta) - y^{(i)}\\big)^2$$ \n",
    "with respect to the parameter $\\theta_j$, where the hypothesis $h(x;\\theta)$ is given by the linear model \n",
    "$$ h(x;\\theta) = \\theta^T x$$\n",
    "\n",
    "Please show all your steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "\n",
    "$$ \\delta J/ \\delta \\theta_j = \\frac{\\delta}{\\delta \\theta_j} \\frac{1}{2m} (h_\\theta(x) - y)^2$$ \n",
    "$$ \\delta J/ \\delta \\theta_j = 2* \\frac{1}{2} (h_\\theta(x) - y) \\frac{\\delta}{\\delta \\theta_j}(h_\\theta(x)-y)$$\n",
    "$$ \\delta J/ \\delta \\theta_j = (h_\\theta(x) - y) \\frac{\\delta}{\\delta \\theta_j}\\sum_{i = 0}^{n} \\big(\\theta_ix_i-y\\big)$$\n",
    "$$ \\delta h_\\theta / \\delta \\theta_j = x_j$$\n",
    "$$ \\delta J/ \\theta_j = (h_\\theta(x) - y)x_j$$\n",
    "            \n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3\\.2 [10pt]**\n",
    "Write a mathematical formulation of how gradient descent will adjust $\\theta_j$ values to minimize the cost $J(Î¸)$. Then, describe each term in the formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "\n",
    "$ \\theta_j := \\theta_j - \\alpha \\frac{\\delta}{\\delta \\theta_j} J(\\theta)$\n",
    "\n",
    "$ \\theta_j$ is the old value that needs to be updated\n",
    "\n",
    "$ \\alpha$ is the learning rate, and determines how fast $\\theta$ changes\n",
    "\n",
    "$ - \\frac{\\delta}{\\delta \\theta_j} J(\\theta)$ determines the direction that the gradient descent goes in\n",
    "            \n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3\\.3 [10pt]**\n",
    "If $x^{(i)} \\in R^4$, and $m=3$, draw the tensors $x$, $\\theta$, $h$, in $ h(x;\\theta) = \\theta^T x$, together with the ground-truth tensor $y$, specifying the dimensions of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "\n",
    "$\\theta = [\\theta_0;\\theta_1;\\theta_2;\\theta_3]$ 4x1\n",
    "\n",
    "$\\theta^T = [\\theta_0,\\theta_1,\\theta_2,\\theta_3]$ 1x4\n",
    "\n",
    "$x = [x_1^{(i)}; x_2^{(i)}; x_3^{(i)}; x_4^{(i)}]$ 4x1\n",
    "\n",
    "$ h(x;\\theta) = \\theta^Tx$\n",
    "\n",
    "$ h(x;\\theta) = [\\theta_0,\\theta_1,\\theta_2,\\theta_3] * [x_1^{(i)}; x_2^{(i)}; x_3^{(i)}; x_4^{(i)}]$ 1x4 * 4x1 = 1x1\n",
    "\n",
    "$ y = [y]$ 1x1, same size as the result of h so you can compare to the ground truth\n",
    "            \n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3\\.4 [2pt]**\n",
    "How do we know that gradient descent has converged?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "\n",
    "We know that gradient descent has converged when the theta value is not impoving anymore. \n",
    "            \n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3\\.5 [6pt]**\n",
    "Write an L2 regularized form of $$ J(\\theta) = \\frac{1}{2m} \\sum_{i = 1}^{m} \\big(h(x^{(i)}; \\theta) - y^{(i)}\\big)^2,$$ explain the additional term(s), and explain why the additional term(s) achieve regularization (i.e. help avoid overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i = 1}^{m} \\big(h(x^{(i)}; \\theta) - y^{(i)}\\big)^2-\\lambda \\sum_{j=1}^{n}\\theta_j^2$$\n",
    "\n",
    "The $\\sum_{j=1}^{n}\\theta_j^2$ helps to acheive regularization because it penalizes for larger $\\theta$ values\n",
    "\n",
    "The $\\lambda$ value controls the amount of regularization there is\n",
    "            \n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Cost Function Design for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a classification problem with inputs $x$ and corresponding labels $y$, where $y^{(i)} \\in \\{-1,+1\\}$. We would like to learn a classifier that computes a linear function of the input $h=\\theta^T x$, and predicts $+1$ if $h(x^{(i)}) \\ge 0$, or $-1$ otherwise. \n",
    "\n",
    "**4\\.1 [2pt]**\n",
    "What can be said about the correctness of the classifierâ€™s prediction if $y^{(i)}h(x^{(i)})>0$? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "\n",
    "$y^{(i)}h(x^{(i)})>0$ means that the classifier's prediction is correct. \n",
    "\n",
    "$y^{(i)}$ is the ground truth, while $h(x^{(i)})$ is the predicted result. If both of these match, the result should be 1 (either $1*1 = 1$ or $-1*-1 = 1$). If there is a mismatch, then $y^{(i)}h(x^{(i)})$ would be less than 0.\n",
    "            \n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4\\.2 [2pt]**\n",
    "What if $y^{(i)}h(x^{(i)})<0$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "\n",
    "$y^{(i)}h(x^{(i)})<0$ means that the classifier is incorrect. If the prediction and the ground truth do not match and are incorrect, it would either be $1*-1 = -1$ or $-1*1 = -1$, both of which result in a negative number, so $y^{(i)}h(x^{(i)})<0$.\n",
    "            \n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4\\.3 [2pt]**\n",
    "Is $yh(x)$ a good loss function to minimize? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== ANSWER GOES HERE ===\n",
    "\n",
    "$yh(x)$ is not a good loss function to minimize. This is because it is the product of the prediction and the ground truth, therefore minimizing it would not do anything to increase the accuracy of the prediction. On the other hand, the sum of squared differences is a better cost function to minimize, because it subsequently also minimized the future differences between the prediction and the ground truth. \n",
    "            \n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Numpy\n",
    "**5\\.1 [5pt]**\n",
    "Modify the cell below to return a 5x5 matrix of ones. Put some code there and press `Ctrl+Enter` to execute contents of the cell. You should see something like the output below. [[1]](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.creation.html#arrays-creation) [[2]](https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.array-creation.html#routines-array-creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.ones((5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2 [10pt]** \n",
    "Vectorizing your code is very important to get results in a reasonable time. Let A be a 10x10 matrix and x be a 10-element column vector. Your friend writes the following code. How would you vectorize this code to run without any for loops? Compare execution speed for different values of `n` with [`%timeit`](http://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.44345421]\n",
      " [3.03942056]\n",
      " [2.26288557]\n",
      " [2.61911885]\n",
      " [3.60133263]\n",
      " [3.27067306]\n",
      " [2.07873609]\n",
      " [2.95750008]\n",
      " [2.77857772]\n",
      " [2.39080727]]\n",
      "979 ns Â± 3.21 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each)\n",
      "[[3.44345421]\n",
      " [3.03942056]\n",
      " [2.26288557]\n",
      " [2.61911885]\n",
      " [3.60133263]\n",
      " [3.27067306]\n",
      " [2.07873609]\n",
      " [2.95750008]\n",
      " [2.77857772]\n",
      " [2.39080727]]\n",
      "264 Âµs Â± 2.84 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "def compute_something(A, x):\n",
    "    v = np.zeros((n, 1))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            v[i] += A[i, j] * x[j]\n",
    "    return v\n",
    "\n",
    "\n",
    "            \n",
    "A = np.random.rand(n, n)\n",
    "x = np.random.rand(n, 1)\n",
    "print(np.matmul(A,x))\n",
    "%timeit np.matmul(A,x)np.matmul(A,x)np.matmul(A,x)\n",
    "print(compute_something(A, x))\n",
    "%timeit compute_something(A, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.44345421]\n",
      " [3.03942056]\n",
      " [2.26288557]\n",
      " [2.61911885]\n",
      " [3.60133263]\n",
      " [3.27067306]\n",
      " [2.07873609]\n",
      " [2.95750008]\n",
      " [2.77857772]\n",
      " [2.39080727]]\n"
     ]
    }
   ],
   "source": [
    "def vectorized(A, x):\n",
    "    return np.matmul(A,x)\n",
    "\n",
    "print(vectorized(A, x))\n",
    "assert np.max(abs(vectorized(A, x) - compute_something(A, x))) < 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 Âµs Â± 26.5 Âµs per loop (mean Â± std. dev. of 7 runs, 5 loops each)\n",
      "The slowest run took 5.84 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "3.44 Âµs Â± 3.41 Âµs per loop (mean Â± std. dev. of 7 runs, 5 loops each)\n",
      "---\n",
      "431 Âµs Â± 32.1 Âµs per loop (mean Â± std. dev. of 7 runs, 5 loops each)\n",
      "The slowest run took 5.78 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "2.45 Âµs Â± 2.42 Âµs per loop (mean Â± std. dev. of 7 runs, 5 loops each)\n",
      "---\n",
      "26.2 ms Â± 825 Âµs per loop (mean Â± std. dev. of 7 runs, 5 loops each)\n",
      "The slowest run took 211.40 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "72.5 Âµs Â± 172 Âµs per loop (mean Â± std. dev. of 7 runs, 5 loops each)\n",
      "---\n",
      "651 ms Â± 1.89 ms per loop (mean Â± std. dev. of 7 runs, 5 loops each)\n",
      "The slowest run took 106.93 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "244 Âµs Â± 560 Âµs per loop (mean Â± std. dev. of 7 runs, 5 loops each)\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for n in [5, 10, 100, 500]:\n",
    "    A = np.random.rand(n, n)\n",
    "    x = np.random.rand(n, 1)\n",
    "    %timeit -n 5 compute_something(A, x)\n",
    "    %timeit -n 5 vectorized(A, x)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
